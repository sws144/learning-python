{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 101 Pandas Exercises\n",
    "https://www.machinelearningplus.com/python/101-pandas-exercises-python/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.1\n",
      "{'system': {'commit': None, 'python': '3.7.6.final.0', 'python-bits': 64, 'OS': 'Windows', 'OS-release': '10', 'machine': 'AMD64', 'processor': 'Intel64 Family 6 Model 158 Stepping 9, GenuineIntel', 'byteorder': 'little', 'LC_ALL': 'None', 'LANG': 'None', 'LOCALE': 'None.None'}, 'dependencies': {'pandas': '1.0.1', 'numpy': '1.18.1', 'pytz': '2019.3', 'dateutil': '2.8.1', 'pip': '20.0.2', 'setuptools': '45.2.0.post20200210', 'Cython': '0.29.15', 'pytest': '5.3.5', 'hypothesis': '5.5.4', 'sphinx': '2.4.0', 'blosc': None, 'feather': None, 'xlsxwriter': '1.2.7', 'lxml.etree': '4.5.0', 'html5lib': '1.0.1', 'pymysql': None, 'psycopg2': None, 'jinja2': '2.11.1', 'IPython': '7.12.0', 'pandas_datareader': None, 'bs4': '4.8.2', 'bottleneck': '1.3.2', 'fastparquet': None, 'gcsfs': None, 'matplotlib': '3.1.3', 'numexpr': '2.7.1', 'odfpy': None, 'openpyxl': '3.0.3', 'pandas_gbq': None, 'pyarrow': None, 'pytables': None, 'pyxlsb': None, 's3fs': None, 'scipy': '1.4.1', 'sqlalchemy': '1.3.13', 'tables': '3.6.1', 'tabulate': None, 'xarray': None, 'xlrd': '1.2.0', 'xlwt': '1.3.0', 'numba': '0.48.0'}}\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "print(pd.__version__)\n",
    "print(pd.show_versions(as_json=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Series from list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "mylist = list('abcedfghijklmnopqrstuvwxyz')\n",
    "myarr = np.arange(26)\n",
    "mydict = dict(zip(mylist, myarr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_mylist = pd.Series(mylist)\n",
    "series_myarr = pd.Series(myarr)\n",
    "series_mydict  = pd.Series(mydict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the index of a series into a column of a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "mylist = list('abcedfghijklmnopqrstuvwxyz')\n",
    "myarr = np.arange(26)\n",
    "mydict = dict(zip(mylist, myarr))\n",
    "ser = pd.Series(mydict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m/\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "zip(*iterables) --> zip object\n",
       "\n",
       "Return a zip object whose .__next__() method returns a tuple where\n",
       "the i-th element comes from the i-th iterable argument.  The .__next__()\n",
       "method continues until the shortest iterable in the argument sequence\n",
       "is exhausted and then it raises StopIteration.\n",
       "\u001b[1;31mType:\u001b[0m           type\n",
       "\u001b[1;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  index  0\n",
      "0     a  0\n",
      "1     b  1\n",
      "2     c  2\n",
      "3     e  3\n",
      "4     d  4\n"
     ]
    }
   ],
   "source": [
    "df_ser = pd.DataFrame(ser).reset_index()\n",
    "print(df_ser.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m/\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "dict() -> new empty dictionary\n",
       "dict(mapping) -> new dictionary initialized from a mapping object's\n",
       "    (key, value) pairs\n",
       "dict(iterable) -> new dictionary initialized as if via:\n",
       "    d = {}\n",
       "    for k, v in iterable:\n",
       "        d[k] = v\n",
       "dict(**kwargs) -> new dictionary initialized with the name=value pairs\n",
       "    in the keyword argument list.  For example:  dict(one=1, two=2)\n",
       "\u001b[1;31mType:\u001b[0m           type\n",
       "\u001b[1;31mSubclasses:\u001b[0m     OrderedDict, defaultdict, Counter, _EnumDict, Bunch, Config, Struct, ColorSchemeTable, StgDict, _CharSizesCache, ...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to combine many series to form a dataframe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "ser1 = pd.Series(list('abcedfghijklmnopqrstuvwxyz'))\n",
    "ser2 = pd.Series(np.arange(26))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ser1n2 = pd.DataFrame({'co1' : ser1, 'col2' : ser2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m\n",
       "\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mindex\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mCollection\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mcolumns\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mCollection\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mForwardRef\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ExtensionDtype'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mcopy\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "Two-dimensional, size-mutable, potentially heterogeneous tabular data.\n",
       "\n",
       "Data structure also contains labeled axes (rows and columns).\n",
       "Arithmetic operations align on both row and column labels. Can be\n",
       "thought of as a dict-like container for Series objects. The primary\n",
       "pandas data structure.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "data : ndarray (structured or homogeneous), Iterable, dict, or DataFrame\n",
       "    Dict can contain Series, arrays, constants, or list-like objects.\n",
       "\n",
       "    .. versionchanged:: 0.23.0\n",
       "       If data is a dict, column order follows insertion-order for\n",
       "       Python 3.6 and later.\n",
       "\n",
       "    .. versionchanged:: 0.25.0\n",
       "       If data is a list of dicts, column order follows insertion-order\n",
       "       for Python 3.6 and later.\n",
       "\n",
       "index : Index or array-like\n",
       "    Index to use for resulting frame. Will default to RangeIndex if\n",
       "    no indexing information part of input data and no index provided.\n",
       "columns : Index or array-like\n",
       "    Column labels to use for resulting frame. Will default to\n",
       "    RangeIndex (0, 1, 2, ..., n) if no column labels are provided.\n",
       "dtype : dtype, default None\n",
       "    Data type to force. Only a single dtype is allowed. If None, infer.\n",
       "copy : bool, default False\n",
       "    Copy data from inputs. Only affects DataFrame / 2d ndarray input.\n",
       "\n",
       "See Also\n",
       "--------\n",
       "DataFrame.from_records : Constructor from tuples, also record arrays.\n",
       "DataFrame.from_dict : From dicts of Series, arrays, or dicts.\n",
       "read_csv\n",
       "read_table\n",
       "read_clipboard\n",
       "\n",
       "Examples\n",
       "--------\n",
       "Constructing DataFrame from a dictionary.\n",
       "\n",
       ">>> d = {'col1': [1, 2], 'col2': [3, 4]}\n",
       ">>> df = pd.DataFrame(data=d)\n",
       ">>> df\n",
       "   col1  col2\n",
       "0     1     3\n",
       "1     2     4\n",
       "\n",
       "Notice that the inferred dtype is int64.\n",
       "\n",
       ">>> df.dtypes\n",
       "col1    int64\n",
       "col2    int64\n",
       "dtype: object\n",
       "\n",
       "To enforce a single dtype:\n",
       "\n",
       ">>> df = pd.DataFrame(data=d, dtype=np.int8)\n",
       ">>> df.dtypes\n",
       "col1    int8\n",
       "col2    int8\n",
       "dtype: object\n",
       "\n",
       "Constructing DataFrame from numpy ndarray:\n",
       "\n",
       ">>> df2 = pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]),\n",
       "...                    columns=['a', 'b', 'c'])\n",
       ">>> df2\n",
       "   a  b  c\n",
       "0  1  2  3\n",
       "1  4  5  6\n",
       "2  7  8  9\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\users\\sw\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\n",
       "\u001b[1;31mType:\u001b[0m           type\n",
       "\u001b[1;31mSubclasses:\u001b[0m     SubclassedDataFrame\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?pd.DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m\n",
       "\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mobjs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mIterable\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mForwardRef\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'DataFrame'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mForwardRef\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Series'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mHashable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mForwardRef\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'DataFrame'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mForwardRef\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Series'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mjoin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'outer'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mignore_index\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mkeys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mlevels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mverify_integrity\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0msort\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mcopy\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mForwardRef\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'DataFrame'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mForwardRef\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Series'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "Concatenate pandas objects along a particular axis with optional set logic\n",
       "along the other axes.\n",
       "\n",
       "Can also add a layer of hierarchical indexing on the concatenation axis,\n",
       "which may be useful if the labels are the same (or overlapping) on\n",
       "the passed axis number.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "objs : a sequence or mapping of Series or DataFrame objects\n",
       "    If a dict is passed, the sorted keys will be used as the `keys`\n",
       "    argument, unless it is passed, in which case the values will be\n",
       "    selected (see below). Any None objects will be dropped silently unless\n",
       "    they are all None in which case a ValueError will be raised.\n",
       "axis : {0/'index', 1/'columns'}, default 0\n",
       "    The axis to concatenate along.\n",
       "join : {'inner', 'outer'}, default 'outer'\n",
       "    How to handle indexes on other axis (or axes).\n",
       "ignore_index : bool, default False\n",
       "    If True, do not use the index values along the concatenation axis. The\n",
       "    resulting axis will be labeled 0, ..., n - 1. This is useful if you are\n",
       "    concatenating objects where the concatenation axis does not have\n",
       "    meaningful indexing information. Note the index values on the other\n",
       "    axes are still respected in the join.\n",
       "keys : sequence, default None\n",
       "    If multiple levels passed, should contain tuples. Construct\n",
       "    hierarchical index using the passed keys as the outermost level.\n",
       "levels : list of sequences, default None\n",
       "    Specific levels (unique values) to use for constructing a\n",
       "    MultiIndex. Otherwise they will be inferred from the keys.\n",
       "names : list, default None\n",
       "    Names for the levels in the resulting hierarchical index.\n",
       "verify_integrity : bool, default False\n",
       "    Check whether the new concatenated axis contains duplicates. This can\n",
       "    be very expensive relative to the actual data concatenation.\n",
       "sort : bool, default False\n",
       "    Sort non-concatenation axis if it is not already aligned when `join`\n",
       "    is 'outer'.\n",
       "    This has no effect when ``join='inner'``, which already preserves\n",
       "    the order of the non-concatenation axis.\n",
       "\n",
       "    .. versionadded:: 0.23.0\n",
       "    .. versionchanged:: 1.0.0\n",
       "\n",
       "       Changed to not sort by default.\n",
       "\n",
       "copy : bool, default True\n",
       "    If False, do not copy data unnecessarily.\n",
       "\n",
       "Returns\n",
       "-------\n",
       "object, type of objs\n",
       "    When concatenating all ``Series`` along the index (axis=0), a\n",
       "    ``Series`` is returned. When ``objs`` contains at least one\n",
       "    ``DataFrame``, a ``DataFrame`` is returned. When concatenating along\n",
       "    the columns (axis=1), a ``DataFrame`` is returned.\n",
       "\n",
       "See Also\n",
       "--------\n",
       "Series.append : Concatenate Series.\n",
       "DataFrame.append : Concatenate DataFrames.\n",
       "DataFrame.join : Join DataFrames using indexes.\n",
       "DataFrame.merge : Merge DataFrames by indexes or columns.\n",
       "\n",
       "Notes\n",
       "-----\n",
       "The keys, levels, and names arguments are all optional.\n",
       "\n",
       "A walkthrough of how this method fits in with other tools for combining\n",
       "pandas objects can be found `here\n",
       "<https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html>`__.\n",
       "\n",
       "Examples\n",
       "--------\n",
       "Combine two ``Series``.\n",
       "\n",
       ">>> s1 = pd.Series(['a', 'b'])\n",
       ">>> s2 = pd.Series(['c', 'd'])\n",
       ">>> pd.concat([s1, s2])\n",
       "0    a\n",
       "1    b\n",
       "0    c\n",
       "1    d\n",
       "dtype: object\n",
       "\n",
       "Clear the existing index and reset it in the result\n",
       "by setting the ``ignore_index`` option to ``True``.\n",
       "\n",
       ">>> pd.concat([s1, s2], ignore_index=True)\n",
       "0    a\n",
       "1    b\n",
       "2    c\n",
       "3    d\n",
       "dtype: object\n",
       "\n",
       "Add a hierarchical index at the outermost level of\n",
       "the data with the ``keys`` option.\n",
       "\n",
       ">>> pd.concat([s1, s2], keys=['s1', 's2'])\n",
       "s1  0    a\n",
       "    1    b\n",
       "s2  0    c\n",
       "    1    d\n",
       "dtype: object\n",
       "\n",
       "Label the index keys you create with the ``names`` option.\n",
       "\n",
       ">>> pd.concat([s1, s2], keys=['s1', 's2'],\n",
       "...           names=['Series name', 'Row ID'])\n",
       "Series name  Row ID\n",
       "s1           0         a\n",
       "             1         b\n",
       "s2           0         c\n",
       "             1         d\n",
       "dtype: object\n",
       "\n",
       "Combine two ``DataFrame`` objects with identical columns.\n",
       "\n",
       ">>> df1 = pd.DataFrame([['a', 1], ['b', 2]],\n",
       "...                    columns=['letter', 'number'])\n",
       ">>> df1\n",
       "  letter  number\n",
       "0      a       1\n",
       "1      b       2\n",
       ">>> df2 = pd.DataFrame([['c', 3], ['d', 4]],\n",
       "...                    columns=['letter', 'number'])\n",
       ">>> df2\n",
       "  letter  number\n",
       "0      c       3\n",
       "1      d       4\n",
       ">>> pd.concat([df1, df2])\n",
       "  letter  number\n",
       "0      a       1\n",
       "1      b       2\n",
       "0      c       3\n",
       "1      d       4\n",
       "\n",
       "Combine ``DataFrame`` objects with overlapping columns\n",
       "and return everything. Columns outside the intersection will\n",
       "be filled with ``NaN`` values.\n",
       "\n",
       ">>> df3 = pd.DataFrame([['c', 3, 'cat'], ['d', 4, 'dog']],\n",
       "...                    columns=['letter', 'number', 'animal'])\n",
       ">>> df3\n",
       "  letter  number animal\n",
       "0      c       3    cat\n",
       "1      d       4    dog\n",
       ">>> pd.concat([df1, df3], sort=False)\n",
       "  letter  number animal\n",
       "0      a       1    NaN\n",
       "1      b       2    NaN\n",
       "0      c       3    cat\n",
       "1      d       4    dog\n",
       "\n",
       "Combine ``DataFrame`` objects with overlapping columns\n",
       "and return only those that are shared by passing ``inner`` to\n",
       "the ``join`` keyword argument.\n",
       "\n",
       ">>> pd.concat([df1, df3], join=\"inner\")\n",
       "  letter  number\n",
       "0      a       1\n",
       "1      b       2\n",
       "0      c       3\n",
       "1      d       4\n",
       "\n",
       "Combine ``DataFrame`` objects horizontally along the x axis by\n",
       "passing in ``axis=1``.\n",
       "\n",
       ">>> df4 = pd.DataFrame([['bird', 'polly'], ['monkey', 'george']],\n",
       "...                    columns=['animal', 'name'])\n",
       ">>> pd.concat([df1, df4], axis=1)\n",
       "  letter  number  animal    name\n",
       "0      a       1    bird   polly\n",
       "1      b       2  monkey  george\n",
       "\n",
       "Prevent the result from including duplicate index values with the\n",
       "``verify_integrity`` option.\n",
       "\n",
       ">>> df5 = pd.DataFrame([1], index=['a'])\n",
       ">>> df5\n",
       "   0\n",
       "a  1\n",
       ">>> df6 = pd.DataFrame([2], index=['a'])\n",
       ">>> df6\n",
       "   0\n",
       "a  2\n",
       ">>> pd.concat([df5, df6], verify_integrity=True)\n",
       "Traceback (most recent call last):\n",
       "    ...\n",
       "ValueError: Indexes have overlapping values: ['a']\n",
       "\u001b[1;31mFile:\u001b[0m      c:\\users\\sw\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\n",
       "\u001b[1;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?pd.concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([ser1, ser2], axis = 1) #1 is col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## How to assign name to the series’ index?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser = pd.Series(list('abcedfghijklmnopqrstuvwxyz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser.name = 'alphabets'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    a\n",
       "1    b\n",
       "2    c\n",
       "3    e\n",
       "4    d\n",
       "Name: alphabets, dtype: object"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ser.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to get the items of series A not present in series B?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser1 = pd.Series([1, 2, 3, 4, 5])\n",
    "ser2 = pd.Series([4, 5, 6, 7, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    2\n",
       "2    3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ser1[~ser1.isin(ser2)] # select ser1 by index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to get the items not common to both series A and series B?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser1 = pd.Series([1, 2, 3, 4, 5])\n",
    "ser2 = pd.Series([4, 5, 6, 7, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncom1 = ser1[~ser1.isin(ser2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncom2 = ser2[~ser2.isin(ser1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    2\n",
       "2    3\n",
       "2    6\n",
       "3    7\n",
       "4    8\n",
       "dtype: int64"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uncom1.append(uncom2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  How to get the minimum, 25th percentile, median, 75th, and max of a numeric series?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser = pd.Series(np.random.normal(10, 5, 25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "normal(loc=0.0, scale=1.0, size=None)\n",
       "\n",
       "Draw random samples from a normal (Gaussian) distribution.\n",
       "\n",
       "The probability density function of the normal distribution, first\n",
       "derived by De Moivre and 200 years later by both Gauss and Laplace\n",
       "independently [2]_, is often called the bell curve because of\n",
       "its characteristic shape (see the example below).\n",
       "\n",
       "The normal distributions occurs often in nature.  For example, it\n",
       "describes the commonly occurring distribution of samples influenced\n",
       "by a large number of tiny, random disturbances, each with its own\n",
       "unique distribution [2]_.\n",
       "\n",
       ".. note::\n",
       "    New code should use the ``normal`` method of a ``default_rng()``\n",
       "    instance instead; see `random-quick-start`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "loc : float or array_like of floats\n",
       "    Mean (\"centre\") of the distribution.\n",
       "scale : float or array_like of floats\n",
       "    Standard deviation (spread or \"width\") of the distribution. Must be\n",
       "    non-negative.\n",
       "size : int or tuple of ints, optional\n",
       "    Output shape.  If the given shape is, e.g., ``(m, n, k)``, then\n",
       "    ``m * n * k`` samples are drawn.  If size is ``None`` (default),\n",
       "    a single value is returned if ``loc`` and ``scale`` are both scalars.\n",
       "    Otherwise, ``np.broadcast(loc, scale).size`` samples are drawn.\n",
       "\n",
       "Returns\n",
       "-------\n",
       "out : ndarray or scalar\n",
       "    Drawn samples from the parameterized normal distribution.\n",
       "\n",
       "See Also\n",
       "--------\n",
       "scipy.stats.norm : probability density function, distribution or\n",
       "    cumulative density function, etc.\n",
       "Generator.normal: which should be used for new code.\n",
       "\n",
       "Notes\n",
       "-----\n",
       "The probability density for the Gaussian distribution is\n",
       "\n",
       ".. math:: p(x) = \\frac{1}{\\sqrt{ 2 \\pi \\sigma^2 }}\n",
       "                 e^{ - \\frac{ (x - \\mu)^2 } {2 \\sigma^2} },\n",
       "\n",
       "where :math:`\\mu` is the mean and :math:`\\sigma` the standard\n",
       "deviation. The square of the standard deviation, :math:`\\sigma^2`,\n",
       "is called the variance.\n",
       "\n",
       "The function has its peak at the mean, and its \"spread\" increases with\n",
       "the standard deviation (the function reaches 0.607 times its maximum at\n",
       ":math:`x + \\sigma` and :math:`x - \\sigma` [2]_).  This implies that\n",
       "normal is more likely to return samples lying close to the mean, rather\n",
       "than those far away.\n",
       "\n",
       "References\n",
       "----------\n",
       ".. [1] Wikipedia, \"Normal distribution\",\n",
       "       https://en.wikipedia.org/wiki/Normal_distribution\n",
       ".. [2] P. R. Peebles Jr., \"Central Limit Theorem\" in \"Probability,\n",
       "       Random Variables and Random Signal Principles\", 4th ed., 2001,\n",
       "       pp. 51, 51, 125.\n",
       "\n",
       "Examples\n",
       "--------\n",
       "Draw samples from the distribution:\n",
       "\n",
       ">>> mu, sigma = 0, 0.1 # mean and standard deviation\n",
       ">>> s = np.random.normal(mu, sigma, 1000)\n",
       "\n",
       "Verify the mean and the variance:\n",
       "\n",
       ">>> abs(mu - np.mean(s))\n",
       "0.0  # may vary\n",
       "\n",
       ">>> abs(sigma - np.std(s, ddof=1))\n",
       "0.1  # may vary\n",
       "\n",
       "Display the histogram of the samples, along with\n",
       "the probability density function:\n",
       "\n",
       ">>> import matplotlib.pyplot as plt\n",
       ">>> count, bins, ignored = plt.hist(s, 30, density=True)\n",
       ">>> plt.plot(bins, 1/(sigma * np.sqrt(2 * np.pi)) *\n",
       "...                np.exp( - (bins - mu)**2 / (2 * sigma**2) ),\n",
       "...          linewidth=2, color='r')\n",
       ">>> plt.show()\n",
       "\n",
       "Two-by-four array of samples from N(3, 6.25):\n",
       "\n",
       ">>> np.random.normal(3, 2.5, size=(2, 4))\n",
       "array([[-4.49401501,  4.00950034, -1.81814867,  7.29718677],   # random\n",
       "       [ 0.39924804,  4.68456316,  4.99394529,  4.84057254]])  # random\n",
       "\u001b[1;31mType:\u001b[0m      builtin_function_or_method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?np.random.normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.6021149 ,  8.18406065,  9.536566  , 14.69126318, 20.44130061])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.percentile(ser, q=[0,25,50,75,100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to get frequency counts of unique items of a series?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser = pd.Series(np.take(list('abcdefgh'), np.random.randint(8, size=30)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'raise'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "Take elements from an array along an axis.\n",
       "\n",
       "When axis is not None, this function does the same thing as \"fancy\"\n",
       "indexing (indexing arrays using arrays); however, it can be easier to use\n",
       "if you need elements along a given axis. A call such as\n",
       "``np.take(arr, indices, axis=3)`` is equivalent to\n",
       "``arr[:,:,:,indices,...]``.\n",
       "\n",
       "Explained without fancy indexing, this is equivalent to the following use\n",
       "of `ndindex`, which sets each of ``ii``, ``jj``, and ``kk`` to a tuple of\n",
       "indices::\n",
       "\n",
       "    Ni, Nk = a.shape[:axis], a.shape[axis+1:]\n",
       "    Nj = indices.shape\n",
       "    for ii in ndindex(Ni):\n",
       "        for jj in ndindex(Nj):\n",
       "            for kk in ndindex(Nk):\n",
       "                out[ii + jj + kk] = a[ii + (indices[jj],) + kk]\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "a : array_like (Ni..., M, Nk...)\n",
       "    The source array.\n",
       "indices : array_like (Nj...)\n",
       "    The indices of the values to extract.\n",
       "\n",
       "    .. versionadded:: 1.8.0\n",
       "\n",
       "    Also allow scalars for indices.\n",
       "axis : int, optional\n",
       "    The axis over which to select values. By default, the flattened\n",
       "    input array is used.\n",
       "out : ndarray, optional (Ni..., Nj..., Nk...)\n",
       "    If provided, the result will be placed in this array. It should\n",
       "    be of the appropriate shape and dtype. Note that `out` is always\n",
       "    buffered if `mode='raise'`; use other modes for better performance.\n",
       "mode : {'raise', 'wrap', 'clip'}, optional\n",
       "    Specifies how out-of-bounds indices will behave.\n",
       "\n",
       "    * 'raise' -- raise an error (default)\n",
       "    * 'wrap' -- wrap around\n",
       "    * 'clip' -- clip to the range\n",
       "\n",
       "    'clip' mode means that all indices that are too large are replaced\n",
       "    by the index that addresses the last element along that axis. Note\n",
       "    that this disables indexing with negative numbers.\n",
       "\n",
       "Returns\n",
       "-------\n",
       "out : ndarray (Ni..., Nj..., Nk...)\n",
       "    The returned array has the same type as `a`.\n",
       "\n",
       "See Also\n",
       "--------\n",
       "compress : Take elements using a boolean mask\n",
       "ndarray.take : equivalent method\n",
       "take_along_axis : Take elements by matching the array and the index arrays\n",
       "\n",
       "Notes\n",
       "-----\n",
       "\n",
       "By eliminating the inner loop in the description above, and using `s_` to\n",
       "build simple slice objects, `take` can be expressed  in terms of applying\n",
       "fancy indexing to each 1-d slice::\n",
       "\n",
       "    Ni, Nk = a.shape[:axis], a.shape[axis+1:]\n",
       "    for ii in ndindex(Ni):\n",
       "        for kk in ndindex(Nj):\n",
       "            out[ii + s_[...,] + kk] = a[ii + s_[:,] + kk][indices]\n",
       "\n",
       "For this reason, it is equivalent to (but faster than) the following use\n",
       "of `apply_along_axis`::\n",
       "\n",
       "    out = np.apply_along_axis(lambda a_1d: a_1d[indices], axis, a)\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> a = [4, 3, 5, 7, 6, 8]\n",
       ">>> indices = [0, 1, 4]\n",
       ">>> np.take(a, indices)\n",
       "array([4, 3, 6])\n",
       "\n",
       "In this example if `a` is an ndarray, \"fancy\" indexing can be used.\n",
       "\n",
       ">>> a = np.array(a)\n",
       ">>> a[indices]\n",
       "array([4, 3, 6])\n",
       "\n",
       "If `indices` is not one dimensional, the output also has these dimensions.\n",
       "\n",
       ">>> np.take(a, [[0, 1], [2, 3]])\n",
       "array([[4, 3],\n",
       "       [5, 7]])\n",
       "\u001b[1;31mFile:\u001b[0m      c:\\users\\sw\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\n",
       "\u001b[1;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?np.take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m/\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "Built-in mutable sequence.\n",
       "\n",
       "If no argument is given, the constructor creates a new empty list.\n",
       "The argument must be an iterable if specified.\n",
       "\u001b[1;31mType:\u001b[0m           type\n",
       "\u001b[1;31mSubclasses:\u001b[0m     _HashedSeq, StackSummary, SList, _ImmutableLineList, FormattedText, NodeList, _ExplodedList, Stack, _Accumulator, _ymd, ...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b    7\n",
       "a    5\n",
       "d    5\n",
       "c    4\n",
       "f    4\n",
       "g    3\n",
       "e    2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ser.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to keep only top 2 most frequent values as it is and replace everything else as ‘Other’?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.RandomState(100)\n",
    "ser = pd.Series(np.random.randint(1, 5, [12]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m/\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "RandomState(seed=None)\n",
       "\n",
       "Container for the slow Mersenne Twister pseudo-random number generator.\n",
       "Consider using a different BitGenerator with the Generator container\n",
       "instead.\n",
       "\n",
       "`RandomState` and `Generator` expose a number of methods for generating\n",
       "random numbers drawn from a variety of probability distributions. In\n",
       "addition to the distribution-specific arguments, each method takes a\n",
       "keyword argument `size` that defaults to ``None``. If `size` is ``None``,\n",
       "then a single value is generated and returned. If `size` is an integer,\n",
       "then a 1-D array filled with generated values is returned. If `size` is a\n",
       "tuple, then an array with that shape is filled and returned.\n",
       "\n",
       "**Compatibility Guarantee**\n",
       "\n",
       "A fixed bit generator using a fixed seed and a fixed series of calls to\n",
       "'RandomState' methods using the same parameters will always produce the\n",
       "same results up to roundoff error except when the values were incorrect.\n",
       "`RandomState` is effectively frozen and will only receive updates that\n",
       "are required by changes in the the internals of Numpy. More substantial\n",
       "changes, including algorithmic improvements, are reserved for\n",
       "`Generator`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "seed : {None, int, array_like, BitGenerator}, optional\n",
       "    Random seed used to initialize the pseudo-random number generator or\n",
       "    an instantized BitGenerator.  If an integer or array, used as a seed for\n",
       "    the MT19937 BitGenerator. Values can be any integer between 0 and\n",
       "    2**32 - 1 inclusive, an array (or other sequence) of such integers,\n",
       "    or ``None`` (the default).  If `seed` is ``None``, then the `MT19937`\n",
       "    BitGenerator is initialized by reading data from ``/dev/urandom``\n",
       "    (or the Windows analogue) if available or seed from the clock\n",
       "    otherwise.\n",
       "\n",
       "Notes\n",
       "-----\n",
       "The Python stdlib module \"random\" also contains a Mersenne Twister\n",
       "pseudo-random number generator with a number of methods that are similar\n",
       "to the ones available in `RandomState`. `RandomState`, besides being\n",
       "NumPy-aware, has the advantage that it provides a much larger number\n",
       "of probability distributions to choose from.\n",
       "\n",
       "See Also\n",
       "--------\n",
       "Generator\n",
       "MT19937\n",
       "numpy.random.BitGenerator\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\users\\sw\\anaconda3\\lib\\site-packages\\numpy\\random\\mtrand.cp37-win_amd64.pyd\n",
       "\u001b[1;31mType:\u001b[0m           type\n",
       "\u001b[1;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?np.random.RandomState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "randint(low, high=None, size=None, dtype='l')\n",
       "\n",
       "Return random integers from `low` (inclusive) to `high` (exclusive).\n",
       "\n",
       "Return random integers from the \"discrete uniform\" distribution of\n",
       "the specified dtype in the \"half-open\" interval [`low`, `high`). If\n",
       "`high` is None (the default), then results are from [0, `low`).\n",
       "\n",
       ".. note::\n",
       "    New code should use the ``integers`` method of a ``default_rng()``\n",
       "    instance instead; see `random-quick-start`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "low : int or array-like of ints\n",
       "    Lowest (signed) integers to be drawn from the distribution (unless\n",
       "    ``high=None``, in which case this parameter is one above the\n",
       "    *highest* such integer).\n",
       "high : int or array-like of ints, optional\n",
       "    If provided, one above the largest (signed) integer to be drawn\n",
       "    from the distribution (see above for behavior if ``high=None``).\n",
       "    If array-like, must contain integer values\n",
       "size : int or tuple of ints, optional\n",
       "    Output shape.  If the given shape is, e.g., ``(m, n, k)``, then\n",
       "    ``m * n * k`` samples are drawn.  Default is None, in which case a\n",
       "    single value is returned.\n",
       "dtype : dtype, optional\n",
       "    Desired dtype of the result. All dtypes are determined by their\n",
       "    name, i.e., 'int64', 'int', etc, so byteorder is not available\n",
       "    and a specific precision may have different C types depending\n",
       "    on the platform. The default value is `np.int_`.\n",
       "\n",
       "    .. versionadded:: 1.11.0\n",
       "\n",
       "Returns\n",
       "-------\n",
       "out : int or ndarray of ints\n",
       "    `size`-shaped array of random integers from the appropriate\n",
       "    distribution, or a single such random int if `size` not provided.\n",
       "\n",
       "See Also\n",
       "--------\n",
       "random_integers : similar to `randint`, only for the closed\n",
       "    interval [`low`, `high`], and 1 is the lowest value if `high` is\n",
       "    omitted.\n",
       "Generator.integers: which should be used for new code.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> np.random.randint(2, size=10)\n",
       "array([1, 0, 0, 0, 1, 1, 0, 0, 1, 0]) # random\n",
       ">>> np.random.randint(1, size=10)\n",
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
       "\n",
       "Generate a 2 x 4 array of ints between 0 and 4, inclusive:\n",
       "\n",
       ">>> np.random.randint(5, size=(2, 4))\n",
       "array([[4, 0, 2, 1], # random\n",
       "       [3, 2, 2, 0]])\n",
       "\n",
       "Generate a 1 x 3 array with 3 different upper bounds\n",
       "\n",
       ">>> np.random.randint(1, [3, 5, 10])\n",
       "array([2, 2, 9]) # random\n",
       "\n",
       "Generate a 1 by 3 array with 3 different lower bounds\n",
       "\n",
       ">>> np.random.randint([1, 5, 7], 10)\n",
       "array([9, 8, 7]) # random\n",
       "\n",
       "Generate a 2 by 4 array using broadcasting with dtype of uint8\n",
       "\n",
       ">>> np.random.randint([1, 3, 5, 7], [[10], [20]], dtype=np.uint8)\n",
       "array([[ 8,  6,  9,  7], # random\n",
       "       [ 1, 16,  9, 12]], dtype=uint8)\n",
       "\u001b[1;31mType:\u001b[0m      builtin_function_or_method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?np.random.randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "valcnts = ser.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "Return a sorted copy of an array.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "a : array_like\n",
       "    Array to be sorted.\n",
       "axis : int or None, optional\n",
       "    Axis along which to sort. If None, the array is flattened before\n",
       "    sorting. The default is -1, which sorts along the last axis.\n",
       "kind : {'quicksort', 'mergesort', 'heapsort', 'stable'}, optional\n",
       "    Sorting algorithm. The default is 'quicksort'. Note that both 'stable'\n",
       "    and 'mergesort' use timsort or radix sort under the covers and, in general,\n",
       "    the actual implementation will vary with data type. The 'mergesort' option\n",
       "    is retained for backwards compatibility.\n",
       "\n",
       "    .. versionchanged:: 1.15.0.\n",
       "       The 'stable' option was added.\n",
       "\n",
       "order : str or list of str, optional\n",
       "    When `a` is an array with fields defined, this argument specifies\n",
       "    which fields to compare first, second, etc.  A single field can\n",
       "    be specified as a string, and not all fields need be specified,\n",
       "    but unspecified fields will still be used, in the order in which\n",
       "    they come up in the dtype, to break ties.\n",
       "\n",
       "Returns\n",
       "-------\n",
       "sorted_array : ndarray\n",
       "    Array of the same type and shape as `a`.\n",
       "\n",
       "See Also\n",
       "--------\n",
       "ndarray.sort : Method to sort an array in-place.\n",
       "argsort : Indirect sort.\n",
       "lexsort : Indirect stable sort on multiple keys.\n",
       "searchsorted : Find elements in a sorted array.\n",
       "partition : Partial sort.\n",
       "\n",
       "Notes\n",
       "-----\n",
       "The various sorting algorithms are characterized by their average speed,\n",
       "worst case performance, work space size, and whether they are stable. A\n",
       "stable sort keeps items with the same key in the same relative\n",
       "order. The four algorithms implemented in NumPy have the following\n",
       "properties:\n",
       "\n",
       "=========== ======= ============= ============ ========\n",
       "   kind      speed   worst case    work space   stable\n",
       "=========== ======= ============= ============ ========\n",
       "'quicksort'    1     O(n^2)            0          no\n",
       "'heapsort'     3     O(n*log(n))       0          no\n",
       "'mergesort'    2     O(n*log(n))      ~n/2        yes\n",
       "'timsort'      2     O(n*log(n))      ~n/2        yes\n",
       "=========== ======= ============= ============ ========\n",
       "\n",
       ".. note:: The datatype determines which of 'mergesort' or 'timsort'\n",
       "   is actually used, even if 'mergesort' is specified. User selection\n",
       "   at a finer scale is not currently available.\n",
       "\n",
       "All the sort algorithms make temporary copies of the data when\n",
       "sorting along any but the last axis.  Consequently, sorting along\n",
       "the last axis is faster and uses less space than sorting along\n",
       "any other axis.\n",
       "\n",
       "The sort order for complex numbers is lexicographic. If both the real\n",
       "and imaginary parts are non-nan then the order is determined by the\n",
       "real parts except when they are equal, in which case the order is\n",
       "determined by the imaginary parts.\n",
       "\n",
       "Previous to numpy 1.4.0 sorting real and complex arrays containing nan\n",
       "values led to undefined behaviour. In numpy versions >= 1.4.0 nan\n",
       "values are sorted to the end. The extended sort order is:\n",
       "\n",
       "  * Real: [R, nan]\n",
       "  * Complex: [R + Rj, R + nanj, nan + Rj, nan + nanj]\n",
       "\n",
       "where R is a non-nan real value. Complex values with the same nan\n",
       "placements are sorted according to the non-nan part if it exists.\n",
       "Non-nan values are sorted as before.\n",
       "\n",
       ".. versionadded:: 1.12.0\n",
       "\n",
       "quicksort has been changed to `introsort <https://en.wikipedia.org/wiki/Introsort>`_.\n",
       "When sorting does not make enough progress it switches to\n",
       "`heapsort <https://en.wikipedia.org/wiki/Heapsort>`_.\n",
       "This implementation makes quicksort O(n*log(n)) in the worst case.\n",
       "\n",
       "'stable' automatically chooses the best stable sorting algorithm\n",
       "for the data type being sorted.\n",
       "It, along with 'mergesort' is currently mapped to\n",
       "`timsort <https://en.wikipedia.org/wiki/Timsort>`_\n",
       "or `radix sort <https://en.wikipedia.org/wiki/Radix_sort>`_\n",
       "depending on the data type.\n",
       "API forward compatibility currently limits the\n",
       "ability to select the implementation and it is hardwired for the different\n",
       "data types.\n",
       "\n",
       ".. versionadded:: 1.17.0\n",
       "\n",
       "Timsort is added for better performance on already or nearly\n",
       "sorted data. On random data timsort is almost identical to\n",
       "mergesort. It is now used for stable sort while quicksort is still the\n",
       "default sort if none is chosen. For timsort details, refer to\n",
       "`CPython listsort.txt <https://github.com/python/cpython/blob/3.7/Objects/listsort.txt>`_.\n",
       "'mergesort' and 'stable' are mapped to radix sort for integer data types. Radix sort is an\n",
       "O(n) sort instead of O(n log n).\n",
       "\n",
       ".. versionchanged:: 1.17.0\n",
       "\n",
       "NaT now sorts to the end of arrays for consistency with NaN.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> a = np.array([[1,4],[3,1]])\n",
       ">>> np.sort(a)                # sort along the last axis\n",
       "array([[1, 4],\n",
       "       [1, 3]])\n",
       ">>> np.sort(a, axis=None)     # sort the flattened array\n",
       "array([1, 1, 3, 4])\n",
       ">>> np.sort(a, axis=0)        # sort along the first axis\n",
       "array([[1, 1],\n",
       "       [3, 4]])\n",
       "\n",
       "Use the `order` keyword to specify a field to use when sorting a\n",
       "structured array:\n",
       "\n",
       ">>> dtype = [('name', 'S10'), ('height', float), ('age', int)]\n",
       ">>> values = [('Arthur', 1.8, 41), ('Lancelot', 1.9, 38),\n",
       "...           ('Galahad', 1.7, 38)]\n",
       ">>> a = np.array(values, dtype=dtype)       # create a structured array\n",
       ">>> np.sort(a, order='height')                        # doctest: +SKIP\n",
       "array([('Galahad', 1.7, 38), ('Arthur', 1.8, 41),\n",
       "       ('Lancelot', 1.8999999999999999, 38)],\n",
       "      dtype=[('name', '|S10'), ('height', '<f8'), ('age', '<i4')])\n",
       "\n",
       "Sort by age, then height if ages are equal:\n",
       "\n",
       ">>> np.sort(a, order=['age', 'height'])               # doctest: +SKIP\n",
       "array([('Galahad', 1.7, 38), ('Lancelot', 1.8999999999999999, 38),\n",
       "       ('Arthur', 1.8, 41)],\n",
       "      dtype=[('name', '|S10'), ('height', '<f8'), ('age', '<i4')])\n",
       "\u001b[1;31mFile:\u001b[0m      c:\\users\\sw\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\n",
       "\u001b[1;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?np.sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3    5\n",
      "4    3\n",
      "1    3\n",
      "2    1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(ser.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser[~ser.isin(ser.value_counts().index[:2])] = 'Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         4\n",
      "1     Other\n",
      "2         4\n",
      "3         3\n",
      "4         3\n",
      "5     Other\n",
      "6         3\n",
      "7     Other\n",
      "8         4\n",
      "9     Other\n",
      "10        3\n",
      "11        3\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(ser)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to bin a numeric series to 10 groups of equal size?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser = pd.Series(np.random.random(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     0.334352\n",
      "1     0.239978\n",
      "2     0.843814\n",
      "3     0.889737\n",
      "4     0.177606\n",
      "5     0.342300\n",
      "6     0.597157\n",
      "7     0.261811\n",
      "8     0.927018\n",
      "9     0.854475\n",
      "10    0.017712\n",
      "11    0.355296\n",
      "12    0.259309\n",
      "13    0.453307\n",
      "14    0.554659\n",
      "15    0.339540\n",
      "16    0.922222\n",
      "17    0.999449\n",
      "18    0.679349\n",
      "19    0.714490\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(ser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3rd\n",
       "1    2nd\n",
       "2    8th\n",
       "3    9th\n",
       "4    1st\n",
       "dtype: category\n",
       "Categories (10, object): [1st < 2nd < 3rd < 4th ... 7th < 8th < 9th < 10th]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.qcut(ser, [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "       labels=['1st', '2nd', '3rd', '4th', '5th', '6th', '7th', '8th', '9th', '10th']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     2nd\n",
       "1     1st\n",
       "2     3rd\n",
       "3     4th\n",
       "4     1st\n",
       "5     2nd\n",
       "6     3rd\n",
       "7     1st\n",
       "8     4th\n",
       "9     4th\n",
       "10    1st\n",
       "11    2nd\n",
       "12    1st\n",
       "13    2nd\n",
       "14    3rd\n",
       "15    2nd\n",
       "16    4th\n",
       "17    4th\n",
       "18    3rd\n",
       "19    3rd\n",
       "dtype: category\n",
       "Categories (4, object): [1st < 2nd < 3rd < 4th]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.qcut(ser, [0,0.25,0.5,0.75,1],\n",
    "       labels=['1st', '2nd', '3rd', '4th'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to convert a numpy array to a dataframe of given shape? (L1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser = pd.Series(np.random.randint(1, 10, 35))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 9, 8, 7, 2, 4, 6, 6, 3, 2, 7, 1, 1, 4, 2, 7, 1, 1, 7, 7, 5, 8,\n",
       "       5, 4, 4, 4, 9, 5, 6, 1, 5, 5, 9, 6, 6])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ser.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(ser.values.reshape(7,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0  1  2  3  4\n",
      "0  5  9  8  7  2\n",
      "1  4  6  6  3  2\n",
      "2  7  1  1  4  2\n",
      "3  7  1  1  7  7\n",
      "4  5  8  5  4  4\n",
      "5  4  9  5  6  1\n",
      "6  5  5  9  6  6\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to find the positions of numbers that are multiples of 3 from a series?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser = pd.Series(np.random.randint(1, 10, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    8\n",
      "1    6\n",
      "2    3\n",
      "3    7\n",
      "4    3\n",
      "5    5\n",
      "6    2\n",
      "dtype: int32\n"
     ]
    }
   ],
   "source": [
    "print(ser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [2],\n",
       "       [4]], dtype=int64)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argwhere(ser.values % 3 == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to extract items at given positions from a series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser = pd.Series(list('abcdefghijklmnopqrstuvwxyz'))\n",
    "pos = [0, 4, 8, 14, 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     a\n",
       "4     e\n",
       "8     i\n",
       "14    o\n",
       "20    u\n",
       "dtype: object"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ser[pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     a\n",
       "4     e\n",
       "8     i\n",
       "14    o\n",
       "20    u\n",
       "dtype: object"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ser.take(pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to stack two series vertically and horizontally ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser1 = pd.Series(range(5))\n",
    "ser2 = pd.Series(list('abcde'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0  1\n",
      "0  0  a\n",
      "1  1  b\n",
      "2  2  c\n",
      "3  3  d\n",
      "4  4  e\n"
     ]
    }
   ],
   "source": [
    "df = pd.concat([ser1,ser2], axis = 1)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0\n",
      "1    1\n",
      "2    2\n",
      "3    3\n",
      "4    4\n",
      "0    a\n",
      "1    b\n",
      "2    c\n",
      "3    d\n",
      "4    e\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "df = pd.concat([ser1,ser2], axis = 0)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to get the positions of items of series A in another series B?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser1 = pd.Series([10, 9, 6, 5, 3, 1, 12, 8, 13])\n",
    "ser2 = pd.Series([1, 3, 10, 13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 4, 0, 8]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[pd.Index(ser1).get_loc(i) for i in ser2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 4, 0, 8]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[pd.Index(ser1).get_loc(i) for i in ser2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to compute the mean squared error on a truth and predicted series?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth = pd.Series(range(10))\n",
    "pred = pd.Series(range(10)) + np.random.random(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = np.mean((truth - pred)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3514253624275141\n"
     ]
    }
   ],
   "source": [
    "print(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to convert the first character of each element in a series to uppercase?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser = pd.Series(['how', 'to', 'kick', 'ass?'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     How\n",
       "1      To\n",
       "2    Kick\n",
       "3    Ass?\n",
       "dtype: object"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ser.map(lambda x: x[0].upper() + x[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     How\n",
       "1      To\n",
       "2    Kick\n",
       "3    Ass?\n",
       "dtype: object"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ser.map(lambda x: x.title())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     How\n",
       "1      To\n",
       "2    Kick\n",
       "3    Ass?\n",
       "dtype: object"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series([i.title() for i in ser])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to calculate the number of characters in each word in a series?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser = pd.Series(['how', 'to', 'kick', 'ass?'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3\n",
       "1    2\n",
       "2    4\n",
       "3    4\n",
       "dtype: int64"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ser.map(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to compute difference of differences between consequtive numbers of a series?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser = pd.Series([1, 3, 6, 10, 15, 21, 27, 35])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser2 = ser.shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff  = ser.subtract(ser2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[nan, nan, 1.0, 1.0, 1.0, 1.0, 0.0, 2.0]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ser.diff().diff().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to convert a series of date-strings to a timeseries?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser = pd.Series(['01 Jan 2010', '02-02-2011', '20120303', '2013/04/04', '2014-05-05', '2015-06-06T12:20'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   2010-01-01 00:00:00\n",
       "1   2011-02-02 00:00:00\n",
       "2   2012-03-03 00:00:00\n",
       "3   2013-04-04 00:00:00\n",
       "4   2014-05-05 00:00:00\n",
       "5   2015-06-06 12:20:00\n",
       "dtype: datetime64[ns]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.to_datetime(ser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   2010-01-01 00:00:00\n",
       "1   2011-02-02 00:00:00\n",
       "2   2012-03-03 00:00:00\n",
       "3   2013-04-04 00:00:00\n",
       "4   2014-05-05 00:00:00\n",
       "5   2015-06-06 12:20:00\n",
       "dtype: datetime64[ns]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dateutil.parser import parse\n",
    "ser.map(lambda x: parse(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to get the day of month, week number, day of year and day of week from a series of date strings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil.parser import parse\n",
    "ser_ts = ser.map(lambda x: parse(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date:  [1, 2, 3, 4, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "print(\"Date: \", ser_ts.dt.day.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date:  [53, 5, 9, 14, 19, 23]\n"
     ]
    }
   ],
   "source": [
    "print(\"Date: \", ser_ts.dt.week.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date:  [2010, 2011, 2012, 2013, 2014, 2015]\n"
     ]
    }
   ],
   "source": [
    "print(\"Date: \", ser_ts.dt.year.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date:  [4, 2, 5, 3, 0, 5]\n"
     ]
    }
   ],
   "source": [
    "print(\"Date: \", ser_ts.dt.weekday.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to convert year-month string to dates corresponding to the 4th day of the month?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser = pd.Series(['Jan 2010', 'Feb 2011', 'Mar 2012'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser_ts = ser.map(lambda x: parse('04 ' + x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   2010-01-04\n",
      "1   2011-02-04\n",
      "2   2012-03-04\n",
      "dtype: datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "print(ser_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to filter words that contain at least 2 vowels from a series?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser = pd.Series(['Apple', 'Orange', 'Plan', 'Python', 'Money'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     Apple\n",
       "1    Orange\n",
       "4     Money\n",
       "dtype: object"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "mask = ser.map(lambda x: sum([Counter(x.lower()).get(i, 0) for i in list('aeiou')]) >= 2)\n",
    "ser[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "mask_two = ser.map(lambda x: sum([Counter(x.lower()).get(i,0) for i in list('aeiou')]) >=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     Apple\n",
       "1    Orange\n",
       "4     Money\n",
       "dtype: object"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ser[mask_two]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to filter valid emails from a series?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "emails = pd.Series(['buying books at amazom.com', 'rameses@egypt.com', 'matt@t.co', 'narendra@modi.com'])\n",
    "pattern ='[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\\\.[A-Za-z]{2,4}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                     []\n",
       "1    [rameses@egypt.com]\n",
       "2            [matt@t.co]\n",
       "3    [narendra@modi.com]\n",
       "dtype: object"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails.str.findall(pattern, flags=re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    rameses@egypt.com\n",
       "2            matt@t.co\n",
       "3    narendra@modi.com\n",
       "dtype: object"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = emails.map(lambda x: bool(re.match(pattern, x)))\n",
    "emails[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rameses@egypt.com', 'matt@t.co', 'narendra@modi.com']"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x[0] for x in [re.findall(pattern, email) for email in emails] if len(x) > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to get the mean of a series grouped by another series?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0]\n",
      "['banana', 'banana', 'apple', 'carrot', 'banana', 'carrot', 'carrot', 'carrot', 'banana', 'carrot']\n"
     ]
    }
   ],
   "source": [
    "fruits = pd.Series(np.random.choice(['apple', 'banana', 'carrot'], 10))\n",
    "weights = pd.Series(np.linspace(1, 10, 10))\n",
    "print(weights.tolist())\n",
    "print(fruits.tolist())\n",
    "#> [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0]\n",
    "#> ['banana', 'carrot', 'apple', 'carrot', 'carrot', 'apple', 'banana', 'carrot', 'apple', 'carrot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "apple     3.00\n",
       "banana    4.25\n",
       "carrot    7.00\n",
       "dtype: float64"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.groupby(fruits).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to compute the euclidean distance between two series?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = pd.Series([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "q = pd.Series([10, 9, 8, 7, 6, 5, 4, 3, 2, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.16590212458495"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum((p-q)**2)**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to find all the local maxima (or peaks) in a numeric series? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser = pd.Series([2, 10, 3, 4, 9, 10, 2, 7, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = np.diff(np.sign(np.diff(ser)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_locs = np.where(dd == -2)[0] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 5, 7], dtype=int64)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peak_locs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiff\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprepend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m<\u001b[0m\u001b[0mno\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mappend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m<\u001b[0m\u001b[0mno\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "Calculate the n-th discrete difference along the given axis.\n",
       "\n",
       "The first difference is given by ``out[i] = a[i+1] - a[i]`` along\n",
       "the given axis, higher differences are calculated by using `diff`\n",
       "recursively.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "a : array_like\n",
       "    Input array\n",
       "n : int, optional\n",
       "    The number of times values are differenced. If zero, the input\n",
       "    is returned as-is.\n",
       "axis : int, optional\n",
       "    The axis along which the difference is taken, default is the\n",
       "    last axis.\n",
       "prepend, append : array_like, optional\n",
       "    Values to prepend or append to `a` along axis prior to\n",
       "    performing the difference.  Scalar values are expanded to\n",
       "    arrays with length 1 in the direction of axis and the shape\n",
       "    of the input array in along all other axes.  Otherwise the\n",
       "    dimension and shape must match `a` except along axis.\n",
       "\n",
       "    .. versionadded:: 1.16.0\n",
       "\n",
       "Returns\n",
       "-------\n",
       "diff : ndarray\n",
       "    The n-th differences. The shape of the output is the same as `a`\n",
       "    except along `axis` where the dimension is smaller by `n`. The\n",
       "    type of the output is the same as the type of the difference\n",
       "    between any two elements of `a`. This is the same as the type of\n",
       "    `a` in most cases. A notable exception is `datetime64`, which\n",
       "    results in a `timedelta64` output array.\n",
       "\n",
       "See Also\n",
       "--------\n",
       "gradient, ediff1d, cumsum\n",
       "\n",
       "Notes\n",
       "-----\n",
       "Type is preserved for boolean arrays, so the result will contain\n",
       "`False` when consecutive elements are the same and `True` when they\n",
       "differ.\n",
       "\n",
       "For unsigned integer arrays, the results will also be unsigned. This\n",
       "should not be surprising, as the result is consistent with\n",
       "calculating the difference directly:\n",
       "\n",
       ">>> u8_arr = np.array([1, 0], dtype=np.uint8)\n",
       ">>> np.diff(u8_arr)\n",
       "array([255], dtype=uint8)\n",
       ">>> u8_arr[1,...] - u8_arr[0,...]\n",
       "255\n",
       "\n",
       "If this is not desirable, then the array should be cast to a larger\n",
       "integer type first:\n",
       "\n",
       ">>> i16_arr = u8_arr.astype(np.int16)\n",
       ">>> np.diff(i16_arr)\n",
       "array([-1], dtype=int16)\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> x = np.array([1, 2, 4, 7, 0])\n",
       ">>> np.diff(x)\n",
       "array([ 1,  2,  3, -7])\n",
       ">>> np.diff(x, n=2)\n",
       "array([  1,   1, -10])\n",
       "\n",
       ">>> x = np.array([[1, 3, 6, 10], [0, 5, 6, 8]])\n",
       ">>> np.diff(x)\n",
       "array([[2, 3, 4],\n",
       "       [5, 1, 2]])\n",
       ">>> np.diff(x, axis=0)\n",
       "array([[-1,  2,  0, -2]])\n",
       "\n",
       ">>> x = np.arange('1066-10-13', '1066-10-16', dtype=np.datetime64)\n",
       ">>> np.diff(x)\n",
       "array([1, 1], dtype='timedelta64[D]')\n",
       "\u001b[1;31mFile:\u001b[0m      c:\\users\\sw\\anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py\n",
       "\u001b[1;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?np.diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to replace missing spaces in a string with the least frequent character?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_str = 'dbc deb abed gade'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser = pd.Series(list('dbc deb abed gade'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = pd.value_counts(ser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d    4\n",
      "     3\n",
      "e    3\n",
      "b    3\n",
      "a    2\n",
      "c    1\n",
      "g    1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "least_freq = freq.dropna().index[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dbcgdebgabedggade'"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\".join(ser.replace(\" \", least_freq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to create a TimeSeries starting ‘2000-01-01’ and 10 weekends (saturdays) after that having random numbers as values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m\n",
       "\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mfastpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "One-dimensional ndarray with axis labels (including time series).\n",
       "\n",
       "Labels need not be unique but must be a hashable type. The object\n",
       "supports both integer- and label-based indexing and provides a host of\n",
       "methods for performing operations involving the index. Statistical\n",
       "methods from ndarray have been overridden to automatically exclude\n",
       "missing data (currently represented as NaN).\n",
       "\n",
       "Operations between Series (+, -, /, *, **) align values based on their\n",
       "associated index values-- they need not be the same length. The result\n",
       "index will be the sorted union of the two indexes.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "data : array-like, Iterable, dict, or scalar value\n",
       "    Contains data stored in Series.\n",
       "\n",
       "    .. versionchanged:: 0.23.0\n",
       "       If data is a dict, argument order is maintained for Python 3.6\n",
       "       and later.\n",
       "\n",
       "index : array-like or Index (1d)\n",
       "    Values must be hashable and have the same length as `data`.\n",
       "    Non-unique index values are allowed. Will default to\n",
       "    RangeIndex (0, 1, 2, ..., n) if not provided. If both a dict and index\n",
       "    sequence are used, the index will override the keys found in the\n",
       "    dict.\n",
       "dtype : str, numpy.dtype, or ExtensionDtype, optional\n",
       "    Data type for the output Series. If not specified, this will be\n",
       "    inferred from `data`.\n",
       "    See the :ref:`user guide <basics.dtypes>` for more usages.\n",
       "name : str, optional\n",
       "    The name to give to the Series.\n",
       "copy : bool, default False\n",
       "    Copy input data.\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\users\\sw\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py\n",
       "\u001b[1;31mType:\u001b[0m           type\n",
       "\u001b[1;31mSubclasses:\u001b[0m     SubclassedSeries\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?pd.Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser = pd.Series(np.random.randint(1,10,10),pd.date_range('2000-01-01',periods=10, freq='W-SAT'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000-01-01    6\n",
      "2000-01-08    1\n",
      "2000-01-15    6\n",
      "2000-01-22    7\n",
      "2000-01-29    3\n",
      "2000-02-05    8\n",
      "2000-02-12    8\n",
      "2000-02-19    6\n",
      "2000-02-26    9\n",
      "2000-03-04    8\n",
      "Freq: W-SAT, dtype: int32\n"
     ]
    }
   ],
   "source": [
    "print(ser)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to fill an intermittent time series so all missing dates show up with values of previous non-missing date?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000-01-01     1.0\n",
      "2000-01-03    10.0\n",
      "2000-01-06     3.0\n",
      "2000-01-08     NaN\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "ser = pd.Series([1,10,3,np.nan], index=pd.to_datetime(['2000-01-01', '2000-01-03', '2000-01-06', '2000-01-08']))\n",
    "print(ser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000-01-01     1.0\n",
       "2000-01-02    10.0\n",
       "2000-01-03    10.0\n",
       "2000-01-04     3.0\n",
       "2000-01-05     3.0\n",
       "2000-01-06     3.0\n",
       "2000-01-07     3.0\n",
       "2000-01-08     3.0\n",
       "Freq: D, dtype: float64"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ser.resample('D').bfill().ffill()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to compute the autocorrelations of a numeric series?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser = pd.Series(np.arange(20) + np.random.normal(1, 10, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      0.033042\n",
      "1     -4.771089\n",
      "2      9.081173\n",
      "3     -8.867776\n",
      "4    -25.337111\n",
      "5     14.150004\n",
      "6    -11.303762\n",
      "7     13.856139\n",
      "8    -10.181481\n",
      "9     10.042040\n",
      "10     7.275705\n",
      "11    13.662240\n",
      "12    30.492711\n",
      "13    27.787851\n",
      "14    12.003419\n",
      "15    15.684655\n",
      "16    22.518100\n",
      "17    22.095415\n",
      "18    10.486459\n",
      "19    27.731315\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(ser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "autocorrelations = [ser.autocorr(i).round(2) for i in range(11)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.29, 0.51, 0.36, 0.38, 0.34, 0.19, 0.33, -0.05, -0.37, 0.37]\n"
     ]
    }
   ],
   "source": [
    "print(autocorrelations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lag having highest correlation:  2\n"
     ]
    }
   ],
   "source": [
    "print('Lag having highest correlation: ', np.argmax(np.abs(autocorrelations[1:]))+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to import only every nth row from a csv file to create a dataframe?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
